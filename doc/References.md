Useful articles
===============

TVM
---

* https://arxiv.org/pdf/1805.08166.pdf
  Learning to Optimize Tensor Programs

* https://arxiv.org/pdf/1802.04799.pdf
  TVM: An Automated End-to-End Optimizing Compiler for Deep Learning

* https://github.com/dmlc/dmlc.github.io/blob/master/\_posts/2016-09-29-build-your-own-tensorflow-with-nnvm-and-torch.markdown
  How about build your own TensorFlow with NNVM and Torch7

* https://github.com/andersy005/tvm-in-action

ML
--

* https://github.com/janishar/mit-deep-learning-book-pdf
  Deep Learning book, updated version, has a chapter about Recurrent Networks
* https://arxiv.org/pdf/1409.2329.pdf
  2015, Zaremba, Recurrent Neural Network Regularization
* https://colah.github.io/posts/2015-08-Understanding-LSTMs/
  2015, Tensorflow Tutorial


AD
--

* http://vertex.ai/blog/fully-automatic-differentiation
* Automatic differentiation for tensor algebras, tech.report, 2017
  https://arxiv.org/pdf/1711.01348
* http://www.columbia.edu/~ahd2125/post/2015/12/5/
  Automatic Differentiation or Mathemagically Finding Derivatives (blog, 2015).
  Contains errors
* https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/
  Introduction to Automatic Differentiation (blog, 2013)
* http://www.autodiff.org/?module=Introduction&submenu=Selected%20Books
  Collection of textbooks on AD
* http://conal.net/papers/beautiful-differentiation/
  Forward-mode AD in Haskell, vector spaces
* http://www.bcl.hamilton.ie/~qobi/stalingrad/
  Reverse-Mode AD in a Functional Framework: Lambda the Ultimate Backpropagator
  Stalingrad (~2005)

Competitors
-----------

* https://www.tensorflow.org/performance/xla/
  XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear
  algebra that optimizes TensorFlow computations.
  - https://haosdent.gitbooks.io/tensorflow-document/content/resources/xla_prerelease.html

* https://github.com/facebookresearch/TensorComprehensions
  A domain specific language to express machine learning workloads.
  - https://arxiv.org/abs/1802.04730
    Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions

* https://github.com/plaidml/plaidml
  PlaidML - PlaidML is the easiest, fastest way to learn and deploy deep
  learning on any device, especially those running macOS or Windows.

* http://dlvm.org/
  - https://arxiv.org/pdf/1711.03016
    DLVM: A modern compiler infrastructure for deep learning systems

* https://github.com/vgvassilev/clad
  - https://llvm.org/devmtg/2013-11/slides/Vassilev-Poster.pdf
    clad - Automatic Differentiation using Clang

Benchmarks
----------

* http://vertex.ai/blog/compiler-comparison
  [By PlaidML] Comparision between PlaidML, TVM, TensorComprehensions

* https://github.com/plaidml/plaidbench/tree/tensorcomp
  [By PlaidML] Benchmarks for Keras kernels, compares TVM and TC

* https://github.com/u39kun/deep-learning-benchmark

* https://knowm.org/deep-learning-frameworks-hands-on-review/
  General ML frameworks Review

Related
-------

* https://arxiv.org/pdf/1805.00907.pdf
  Glow: Graph Lowering Compiler Techniques for Neural Networks

* https://en.wikipedia.org/wiki/Polytope\_model

