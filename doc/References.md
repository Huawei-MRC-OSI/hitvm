Useful articles
===============

ML
--

### General

* https://github.com/janishar/mit-deep-learning-book-pdf
  Deep Learning book, updated version, has a chapter about Recurrent Networks
* https://arxiv.org/pdf/1409.2329.pdf
  2015, Zaremba, Recurrent Neural Network Regularization
* https://colah.github.io/posts/2015-08-Understanding-LSTMs/
  2015, Tensorflow Tutorial
* https://arxiv.org/pdf/1706.02021.pdf
  Network Sketching: Exploiting Binary Structure in Deep CNNs
  (Note: especially section 4)

### NLP

Generic articles:

* https://arxiv.org/pdf/1708.02709.pdf
  Recent Trends in Deep Learning Based Natural Language Processing

Occurences in mdeia:

* https://pdfs.semanticscholar.org/presentation/51d9/81c1b28818fd0ee94dd3e607e1004874dfef.pdf
  2015, Research on Deep Learning for Natural Language Processing at Huawei Noahâ€™s Ark Lab
* https://www.huawei.com/en/about-huawei/publications/winwin-magazine/AI/intelligent-agents-tomorrow-digital-valets
  2016, Nuawei NLP news
* http://www.aclweb.org/anthology/N16-4004
  Noah ark document

### OCR

* https://hackernoon.com/latest-deep-learning-ocr-with-keras-and-supervisely-in-15-minutes-34aecd630ed8
  2017, some Optical Character Recognition state-of-the-art article.
* https://arxiv.org/pdf/1805.09441.pdf
  2018, Implicit Language Model in LSTM for OCR
* https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6628705
  2013, High-Performance OCR for Printed English and Fraktur using LSTM Networks
  - https://sourceforge.net/projects/rnnl/
    RNNLib - OpenSource library which was used by the authors.


### AD

* http://vertex.ai/blog/fully-automatic-differentiation
* Automatic differentiation for tensor algebras, tech.report, 2017
  https://arxiv.org/pdf/1711.01348
* http://www.columbia.edu/~ahd2125/post/2015/12/5/
  Automatic Differentiation or Mathemagically Finding Derivatives (blog, 2015).
  Contains errors
* https://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/
  Introduction to Automatic Differentiation (blog, 2013)
* http://www.autodiff.org/?module=Introduction&submenu=Selected%20Books
  Collection of textbooks on AD
* http://conal.net/papers/beautiful-differentiation/
  Forward-mode AD in Haskell, vector spaces
* http://www.bcl.hamilton.ie/~qobi/stalingrad/
  Reverse-Mode AD in a Functional Framework: Lambda the Ultimate Backpropagator
  Stalingrad (~2005)
* https://arxiv.org/pdf/1806.02136.pdf
  Efficient Differentiable Programming in a Functional
  Array-Processing Language
* https://people.csail.mit.edu/tzumao/gradient_halide/gradient_halide.pdf
  Differentiable Programming for
  Image Processing and Deep Learning in Halide


TVM
---

### General

* https://arxiv.org/pdf/1805.08166.pdf
  Learning to Optimize Tensor Programs

* https://arxiv.org/pdf/1802.04799.pdf
  TVM: An Automated End-to-End Optimizing Compiler for Deep Learning

* https://github.com/dmlc/dmlc.github.io/blob/master/\_posts/2016-09-29-build-your-own-tensorflow-with-nnvm-and-torch.markdown
  How about build your own TensorFlow with NNVM and Torch7

* https://github.com/andersy005/tvm-in-action

### Competitors

* https://www.tensorflow.org/performance/xla/
  XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear
  algebra that optimizes TensorFlow computations.
  - https://haosdent.gitbooks.io/tensorflow-document/content/resources/xla_prerelease.html

* https://github.com/facebookresearch/TensorComprehensions
  A domain specific language to express machine learning workloads.
  - https://arxiv.org/abs/1802.04730
    Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions

* https://github.com/plaidml/plaidml
  PlaidML - PlaidML is the easiest, fastest way to learn and deploy deep
  learning on any device, especially those running macOS or Windows.

* http://dlvm.org/
  - https://arxiv.org/pdf/1711.03016
    DLVM: A modern compiler infrastructure for deep learning systems

* https://github.com/vgvassilev/clad
  - https://llvm.org/devmtg/2013-11/slides/Vassilev-Poster.pdf
    clad - Automatic Differentiation using Clang

### Benchmarks

* https://github.com/dmlc/tvm/wiki/Benchmark
  TVM Benchmarking WIKI (remote devices for now)

* http://vertex.ai/blog/compiler-comparison
  [By PlaidML] Comparision between PlaidML, TVM, TensorComprehensions

* https://github.com/plaidml/plaidbench/tree/tensorcomp
  [By PlaidML] Benchmarks for Keras kernels, compares TVM and TC

* https://github.com/u39kun/deep-learning-benchmark

* https://knowm.org/deep-learning-frameworks-hands-on-review/
  General ML frameworks Review

### Related

* https://arxiv.org/pdf/1805.00907.pdf
  Glow: Graph Lowering Compiler Techniques for Neural Networks

* https://en.wikipedia.org/wiki/Polytope\_model

