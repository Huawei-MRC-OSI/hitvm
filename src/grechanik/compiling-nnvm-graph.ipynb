{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling a nn like in the nnvm tutorial\n",
    "\n",
    "Let's just define some nn and compile it. After that we'll look into nnvm passes which are applied during this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnvm.compiler\n",
    "import nnvm.symbol as sym\n",
    "import tvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some simple neural network-like graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sym.Variable(\"x\")\n",
    "w = sym.Variable(\"w\")\n",
    "b = sym.Variable(\"b\")\n",
    "z = sym.conv2d(data=x, weight=w, bias=b, channels=3, kernel_size=(5,5))\n",
    "z = z + sym.conv2d(data=z, weight=w, bias=b, channels=3, kernel_size=(5,5), padding=[2,2])\n",
    "\n",
    "compute_graph = nnvm.graph.create(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the graph in human-readable form with the `ir` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(%x, %w, %b) {\n",
      "  %3 = conv2d(%x, %w, %b, kernel_size='(5, 5)', channels='3')\n",
      "  %4 = conv2d(%3, %w, %b, channels='3', padding='[2, 2]', kernel_size='(5, 5)')\n",
      "  %5 = broadcast_add(%3, %4)\n",
      "  ret %5\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(compute_graph.ir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the graph, let's perform an ugly trick and modify the function `Graph.apply`, which applies a list of passes to a graph, so that it print the applied passes and the resulting graph. It will help us to figure out what the building process consists of. Note though that this won't show some passes which are called from C++ code (like PlanMemory which is applied inside GraphFuseCompile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_graph_apply = nnvm.graph.Graph.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_modified_graph_apply(self, passes):\n",
    "    res = old_graph_apply(self, passes)\n",
    "    # printing ir is also implemented as a pass, so we have to prevent an infinite loop\n",
    "    if passes not in [[\"PrintGraphIR\"], \"PrintGraphIR\", [\"SaveJSON\"], \"SaveJSON\"]:\n",
    "        print(\"Applied passes \" + str(passes))\n",
    "        print(res.ir())\n",
    "    return res\n",
    "\n",
    "nnvm.graph.Graph.apply = my_modified_graph_apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call the build function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied passes CorrectLayout\n",
      "Graph(%x, %w, %b) {\n",
      "  %3 = conv2d(%x, %w, %b, kernel_size='(5, 5)', channels='3')\n",
      "  %4 = conv2d(%3, %w, %b, channels='3', padding='[2, 2]', kernel_size='(5, 5)')\n",
      "  %5 = broadcast_add(%3, %4)\n",
      "  ret %5\n",
      "}\n",
      "graph_attr_keys = [layout]\n",
      "\n",
      "Applied passes InferShape\n",
      "Graph(%x, %w, %b) {\n",
      "  %3 = conv2d(%x, %w, %b, kernel_size='(5, 5)', channels='3')\n",
      "  %4 = conv2d(%3, %w, %b, channels='3', padding='[2, 2]', kernel_size='(5, 5)')\n",
      "  %5 = broadcast_add(%3, %4)\n",
      "  ret %5\n",
      "}\n",
      "graph_attr_keys = [shape_num_unknown_nodes, layout, shape]\n",
      "\n",
      "Applied passes ['InferShape', 'SimplifyInference']\n",
      "Graph(%x, %w, %b) {\n",
      "  %3 = conv2d(%x, %w, %b, kernel_size='(5, 5)', channels='3')\n",
      "  %4 = conv2d(%3, %w, %b, channels='3', padding='[2, 2]', kernel_size='(5, 5)')\n",
      "  %5 = broadcast_add(%3, %4)\n",
      "  ret %5\n",
      "}\n",
      "Applied passes InferShape\n",
      "Graph(%x, %w, %b) {\n",
      "  %3 = conv2d(%x, %w, %b, kernel_size='(5, 5)', channels='3')\n",
      "  %4 = conv2d(%3, %w, %b, channels='3', padding='[2, 2]', kernel_size='(5, 5)')\n",
      "  %5 = broadcast_add(%3, %4)\n",
      "  ret %5\n",
      "}\n",
      "graph_attr_keys = [shape_num_unknown_nodes, shape]\n",
      "\n",
      "Applied passes InferShape\n",
      "Graph(%x, %w, %b) {\n",
      "  %3 = conv2d(%x, %w, %b, kernel_size='(5, 5)', channels='3')\n",
      "  %4 = conv2d(%3, %w, %b, channels='3', padding='[2, 2]', kernel_size='(5, 5)')\n",
      "  %5 = broadcast_add(%3, %4)\n",
      "  ret %5\n",
      "}\n",
      "graph_attr_keys = [shape, opt_level, target, dtype_inputs, shape_num_unknown_nodes]\n",
      "\n",
      "Applied passes InferType\n",
      "Graph(%x, %w, %b) {\n",
      "  %3 = conv2d(%x, %w, %b, kernel_size='(5, 5)', channels='3')\n",
      "  %4 = conv2d(%3, %w, %b, channels='3', padding='[2, 2]', kernel_size='(5, 5)')\n",
      "  %5 = broadcast_add(%3, %4)\n",
      "  ret %5\n",
      "}\n",
      "graph_attr_keys = [dtype_num_unknown_nodes, dtype, shape, opt_level, target, shape_num_unknown_nodes]\n",
      "\n",
      "Applied passes GraphFusePartition\n",
      "Graph(%x, %w, %b) {\n",
      "  %3 = conv2d(%x, %w, %b, kernel_size='(5, 5)', channels='3')\n",
      "  %4 = conv2d(%3, %w, %b, channels='3', padding='[2, 2]', kernel_size='(5, 5)')\n",
      "  %5 = broadcast_add(%3, %4)\n",
      "  ret %5\n",
      "}\n",
      "graph_attr_keys = [pattern, group_master, target, group_root, shape_num_unknown_nodes, dtype, shape, dtype_num_unknown_nodes]\n",
      "\n",
      "Applied passes GraphFuseCompile\n",
      "Graph(%x, %w, %b) {\n",
      "  %3 = tvm_op(%x, %w, %b, num_outputs='1', num_inputs='3', flatten_data='0', func_name='fuse_conv2d')\n",
      "  %4 = tvm_op(%3, %w, %b, num_outputs='1', num_inputs='3', flatten_data='0', func_name='fuse_conv2d_broadcast_add')\n",
      "  ret %4\n",
      "}\n",
      "graph_attr_keys = [storage_id, dtype, dltype, shape, module]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with nnvm.compiler.build_config(opt_level=2):\n",
    "    deploy_graph, lib, params = nnvm.compiler.build(\n",
    "        compute_graph, target=\"llvm\", shape={\"x\": (7,3,11,13)}, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of output. First the graph is transformed by some passes. Then in the pass GraphFuseCompile, lowering is performed. If you look as the source code, you'll see that it calls some function `\"nnvm.compiler.lower\"` dynamically by its name. It's actually called `_lower` and it can be found in `nnvm/python/nnvm/compiler/build_module.py`. It calls `tvm.lower` which does stuff (here is the boundary between nnvm and tvm!) and, if the logging level is debug, it also prints out some lowered representation (not exactly the one actually used though). The lowering process is investigated in a separate notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's return the function to its original state\n",
    "nnvm.graph.Graph.apply = old_graph_apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNVM Passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of all passes registered with `NNVM_REGISTER_PASS` (TODO: How to get this list with python?):\n",
    "- AlterOpLayout\n",
    "- CorrectLayout\n",
    "- PlanMemory\n",
    "- InferShape\n",
    "- LoadJSON\n",
    "- SaveJSON\n",
    "- GraphFusePartition\n",
    "- GraphFuseCompile\n",
    "- Gradient\n",
    "- PlaceDevice\n",
    "- OrderMutation\n",
    "- FoldScaleAxis\n",
    "- SimplifyInference\n",
    "- PrecomputePrune\n",
    "- InferShape\n",
    "- InferType\n",
    "- PrintGraphIR\n",
    "\n",
    "Few of them are applied during compilation. In my opinion, the most important ones are GraphFusePartition and GraphFuseCompile. The best source of information on what passes do is tests (or just grep by the pass name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Fusion\n",
    "\n",
    "Let's look at how graph fusion works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's define some neural net. We have to assign shapes and dtypes to some variables because we will perform compilation steps manually. Dtype is a number (TODO: Figure out what type each number means). You may notice that weights can be omitted, in which case they are automatically added as variables with some sensible names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(%x,\n",
      "      %conv2d2_weight,\n",
      "      %conv2d2_bias,\n",
      "      %conv2d3_weight,\n",
      "      %conv2d3_bias,\n",
      "      %dense0_weight,\n",
      "      %dense0_bias) {\n",
      "  %3 = conv2d(%x, %conv2d2_weight, %conv2d2_bias, channels='13', padding='[1, 1]', kernel_size='(3, 3)')\n",
      "  %4 = relu(%3)\n",
      "  %7 = conv2d(%4, %conv2d3_weight, %conv2d3_bias, channels='13', padding='[1, 1]', kernel_size='(3, 3)')\n",
      "  %8 = relu(%7)\n",
      "  %9 = broadcast_add(%x, %8)\n",
      "  %10 = flatten(%9)\n",
      "  %13 = dense(%10, %dense0_weight, %dense0_bias, units='10')\n",
      "  %14 = softmax(%13)\n",
      "  %15 = sum(%14, keepdims='True')\n",
      "  ret %15\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x = sym.Variable(\"x\", shape=(1,13,100,100), dtype=1)\n",
    "z = x\n",
    "z = sym.conv2d(data=z, channels=13, kernel_size=(3,3), padding=[1,1])\n",
    "z = sym.relu(data=z)\n",
    "z = sym.conv2d(data=z, channels=13, kernel_size=(3,3), padding=[1,1])\n",
    "z = sym.relu(data=z)\n",
    "z = x + z\n",
    "z = sym.flatten(z)\n",
    "z = sym.dense(data=z, units=10)\n",
    "z = sym.softmax(data=z)\n",
    "# TODO: Doesn't work without keepdims which might be a bug in nnvm, investigate\n",
    "z = sym.sum(z, keepdims=True)\n",
    "\n",
    "graph = nnvm.graph.create(z)\n",
    "print(graph.ir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to infer shapes and types first (and it's actually may be interesting to look at them). Such passes add information as graph attributes which are usually just lists of node attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of nodes: [[1, 13, 100, 100], [13, 13, 3, 3], [13], [1, 13, 100, 100], [1, 13, 100, 100], [13, 13, 3, 3], [13], [1, 13, 100, 100], [1, 13, 100, 100], [1, 13, 100, 100], [1, 130000], [10, 130000], [10], [1, 10], [1, 10], [1, 1]]\n",
      "types of nodes: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "graph = graph.apply('InferShape').apply('InferType')\n",
    "\n",
    "print(\"shapes of nodes: \" + str(graph.json_attr('shape')))\n",
    "print(\"types of nodes: \" + str(graph.json_attr('dtype')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out nodes of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'attrs': {'dtype': '1', 'shape': '(1, 13, 100, 100)'},\n",
       "  'inputs': [],\n",
       "  'name': 'x',\n",
       "  'op': 'null'},\n",
       " {'attrs': {'channels': '13', 'kernel_size': '(3, 3)', 'padding': '[1, 1]'},\n",
       "  'inputs': [],\n",
       "  'name': 'conv2d2_weight',\n",
       "  'op': 'null'},\n",
       " {'attrs': {'channels': '13', 'kernel_size': '(3, 3)', 'padding': '[1, 1]'},\n",
       "  'inputs': [],\n",
       "  'name': 'conv2d2_bias',\n",
       "  'op': 'null'},\n",
       " {'attrs': {'channels': '13', 'kernel_size': '(3, 3)', 'padding': '[1, 1]'},\n",
       "  'inputs': [[0, 0, 0], [1, 0, 0], [2, 0, 0]],\n",
       "  'name': 'conv2d2',\n",
       "  'op': 'conv2d'},\n",
       " {'inputs': [[3, 0, 0]], 'name': 'relu0', 'op': 'relu'},\n",
       " {'attrs': {'channels': '13', 'kernel_size': '(3, 3)', 'padding': '[1, 1]'},\n",
       "  'inputs': [],\n",
       "  'name': 'conv2d3_weight',\n",
       "  'op': 'null'},\n",
       " {'attrs': {'channels': '13', 'kernel_size': '(3, 3)', 'padding': '[1, 1]'},\n",
       "  'inputs': [],\n",
       "  'name': 'conv2d3_bias',\n",
       "  'op': 'null'},\n",
       " {'attrs': {'channels': '13', 'kernel_size': '(3, 3)', 'padding': '[1, 1]'},\n",
       "  'inputs': [[4, 0, 0], [5, 0, 0], [6, 0, 0]],\n",
       "  'name': 'conv2d3',\n",
       "  'op': 'conv2d'},\n",
       " {'inputs': [[7, 0, 0]], 'name': 'relu1', 'op': 'relu'},\n",
       " {'inputs': [[0, 0, 0], [8, 0, 0]],\n",
       "  'name': '__add_symbol__1',\n",
       "  'op': 'broadcast_add'},\n",
       " {'inputs': [[9, 0, 0]], 'name': 'flatten0', 'op': 'flatten'},\n",
       " {'attrs': {'units': '10'},\n",
       "  'inputs': [],\n",
       "  'name': 'dense0_weight',\n",
       "  'op': 'null'},\n",
       " {'attrs': {'units': '10'}, 'inputs': [], 'name': 'dense0_bias', 'op': 'null'},\n",
       " {'attrs': {'units': '10'},\n",
       "  'inputs': [[10, 0, 0], [11, 0, 0], [12, 0, 0]],\n",
       "  'name': 'dense0',\n",
       "  'op': 'dense'},\n",
       " {'inputs': [[13, 0, 0]], 'name': 'softmax0', 'op': 'softmax'},\n",
       " {'attrs': {'keepdims': 'True'},\n",
       "  'inputs': [[14, 0, 0]],\n",
       "  'name': 'sum0',\n",
       "  'op': 'sum'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.index.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th node:\t{'op': 'flatten', 'name': 'flatten0', 'inputs': [[9, 0, 0]]}\n",
      "and its shape:\t[1, 130000]\n"
     ]
    }
   ],
   "source": [
    "print(\"10th node:\\t\" + str(graph.index.nodes[10]))\n",
    "print(\"and its shape:\\t\" + str(graph.json_attr('shape')[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's partition the graph and fuse its parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph.apply('GraphFusePartition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition is done according to certain rules which can be found in one of the papers about tvm/nnvm. All operation are categorized into several categories (\"patterns\"): ElemWise, Broadcast, Injective, CommReduce, OutEWiseFusable, Opaque. The rules control whether two operations belonging to some categories can be fused together and what will be the category of the resulting fused operation. Operators are categorized in the python part of code, grep by `register_pattern`\n",
    "\n",
    "TODO: What are these? What are root and master nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots: [0, 1, 2, 4, 4, 5, 6, 9, 9, 9, 10, 11, 12, 13, 14, 15]\n",
      "masters: [-1, -1, -1, 3, 3, -1, -1, 7, 7, 7, 10, -1, -1, 13, 14, 15]\n",
      "\n",
      "x has root x and master None\n",
      "conv2d2_weight has root conv2d2_weight and master None\n",
      "conv2d2_bias has root conv2d2_bias and master None\n",
      "conv2d2 has root relu0 and master conv2d2\n",
      "relu0 has root relu0 and master conv2d2\n",
      "conv2d3_weight has root conv2d3_weight and master None\n",
      "conv2d3_bias has root conv2d3_bias and master None\n",
      "conv2d3 has root __add_symbol__1 and master conv2d3\n",
      "relu1 has root __add_symbol__1 and master conv2d3\n",
      "__add_symbol__1 has root __add_symbol__1 and master conv2d3\n",
      "flatten0 has root flatten0 and master flatten0\n",
      "dense0_weight has root dense0_weight and master None\n",
      "dense0_bias has root dense0_bias and master None\n",
      "dense0 has root dense0 and master dense0\n",
      "softmax0 has root softmax0 and master softmax0\n",
      "sum0 has root sum0 and master sum0\n"
     ]
    }
   ],
   "source": [
    "print(\"roots: \" + str(graph.json_attr('group_root')))\n",
    "print(\"masters: \" + str(graph.json_attr('group_master')))\n",
    "print()\n",
    "\n",
    "for n, r, m in zip(graph.index.nodes, graph.json_attr('group_root'), graph.json_attr('group_master')):\n",
    "    print(n['name'] + \n",
    "          \" has root \" + (graph.index.nodes[r]['name'] if r >= 0 else \"None\") + \n",
    "          \" and master \" + (graph.index.nodes[m]['name'] if m >= 0 else \"None\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now to understand how the graph was partitioned, let's just call GraphFuseCompile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph._set_json_attr(\"target\", \"llvm\", \"str\")\n",
    "with tvm.target.create(\"llvm\"):\n",
    "    compiled = graph.apply('GraphFuseCompile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(%x,\n",
      "      %conv2d2_weight,\n",
      "      %conv2d2_bias,\n",
      "      %conv2d3_weight,\n",
      "      %conv2d3_bias,\n",
      "      %dense0_weight,\n",
      "      %dense0_bias) {\n",
      "  %3 = tvm_op(%x, %conv2d2_weight, %conv2d2_bias, num_outputs='1', num_inputs='3', flatten_data='0', func_name='fuse_conv2d_relu')\n",
      "  %6 = tvm_op(%x, %3, %conv2d3_weight, %conv2d3_bias, num_outputs='1', num_inputs='4', flatten_data='0', func_name='fuse_conv2d_relu_broadcast_add')\n",
      "  %7 = tvm_op(%6, num_outputs='1', num_inputs='1', flatten_data='0', func_name='fuse_flatten')\n",
      "  %10 = tvm_op(%7, %dense0_weight, %dense0_bias, num_outputs='1', num_inputs='3', flatten_data='0', func_name='fuse_dense')\n",
      "  %11 = tvm_op(%10, num_outputs='1', num_inputs='1', flatten_data='0', func_name='fuse_softmax')\n",
      "  %12 = tvm_op(%11, num_outputs='1', num_inputs='1', flatten_data='0', func_name='fuse_sum')\n",
      "  ret %12\n",
      "}\n",
      "graph_attr_keys = [storage_id, dtype, dltype, shape, module]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(compiled.ir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you can guess by the func_names which operations have been fused. Not much, to my taste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a pass called Gradient, so let's compute some gradients. Actually, there is a nice python function which passes needed parameters to this pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnvm.compiler.graph_util import gradients, get_gradient_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our previous graph contains broadcast_add whose gradients were not implemented at the time of writing. So let's define another nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sym.Variable(\"x\", shape=(1,13,100,100), dtype=1)\n",
    "z = x\n",
    "z = sym.conv2d(data=z, channels=13, kernel_size=(3,3), padding=[1,1])\n",
    "z = sym.relu(data=z)\n",
    "z = sym.flatten(z)\n",
    "z = sym.dense(data=z, units=10)\n",
    "z = sym.softmax(data=z)\n",
    "z = sym.sum(z, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a graph returning gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(%x,\n",
      "      %conv2d4_weight,\n",
      "      %conv2d4_bias,\n",
      "      %dense1_weight,\n",
      "      %dense1_bias) {\n",
      "  %3 = conv2d(%x, %conv2d4_weight, %conv2d4_bias, channels='13', padding='[1, 1]', kernel_size='(3, 3)')\n",
      "  %4 = relu(%3)\n",
      "  %5 = flatten(%4)\n",
      "  %8 = dense(%5, %dense1_weight, %dense1_bias, units='10')\n",
      "  %9 = softmax(%8)\n",
      "  %10 = sum(%9, keepdims='True')\n",
      "  %11 = ones_like(%10)\n",
      "  %12 = expand_like(%11, %9, exclude='0', axis='[]')\n",
      "  %13 = elemwise_mul(%12, %9)\n",
      "  %14 = sum(%13, keepdims='true', axis='-1')\n",
      "  %15 = broadcast_mul(%14, %9)\n",
      "  %16 = elemwise_sub(%13, %15)\n",
      "  %17 = matmul(%16, %dense1_weight)\n",
      "  %18 = reshape_like(%17, %4, ='')\n",
      "  %19 = zeros_like(%3)\n",
      "  %20 = greater(%3, %19, exclude='true')\n",
      "  %21 = elemwise_mul(%18, %20)\n",
      "  %22 = _conv2d_grad(%21, %x, %conv2d4_weight, channels='13', padding='[1, 1]', kernel_size='(3, 3)')\n",
      "  ret %22.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "grad_graph = get_gradient_graph(z, x)\n",
    "print(grad_graph.ir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use the function gradients to create symbols computing gradients (behind the scenes this function creates a graph and calls get_gradient_graph). If we want to return several values from a graph, we can group symbols with `nnvm.sym.Group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(%x,\n",
      "      %conv2d4_weight,\n",
      "      %conv2d4_bias,\n",
      "      %dense1_weight,\n",
      "      %dense1_bias) {\n",
      "  %3 = conv2d(%x, %conv2d4_weight, %conv2d4_bias, channels='13', padding='[1, 1]', kernel_size='(3, 3)')\n",
      "  %4 = relu(%3)\n",
      "  %5 = flatten(%4)\n",
      "  %8 = dense(%5, %dense1_weight, %dense1_bias, units='10')\n",
      "  %9 = softmax(%8)\n",
      "  %10 = sum(%9, keepdims='True')\n",
      "  %11 = ones_like(%10)\n",
      "  %12 = expand_like(%11, %9, exclude='0', axis='[]')\n",
      "  %13 = elemwise_mul(%12, %9)\n",
      "  %14 = sum(%13, keepdims='true', axis='-1')\n",
      "  %15 = broadcast_mul(%14, %9)\n",
      "  %16 = elemwise_sub(%13, %15)\n",
      "  %17 = matmul(%16, %dense1_weight)\n",
      "  %18 = reshape_like(%17, %4, ='')\n",
      "  %19 = zeros_like(%3)\n",
      "  %20 = greater(%3, %19, exclude='true')\n",
      "  %21 = elemwise_mul(%18, %20)\n",
      "  %22 = _conv2d_grad(%21, %x, %conv2d4_weight, channels='13', padding='[1, 1]', kernel_size='(3, 3)')\n",
      "  ret %10, %22.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dz_dx = gradients(z, x)\n",
    "# dz_dx is a list containing one symbol, so it looks a bit awckward\n",
    "graph = nnvm.graph.create(nnvm.sym.Group([z] + dz_dx))\n",
    "print(graph.ir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many operation for which gradients are not implemented, here is the list:\n",
    "- `__add_symbol__`\n",
    "- `__div_symbol__`\n",
    "- `__layout_transform__`\n",
    "- `__mul_symbol__`\n",
    "- `__sub_symbol__`\n",
    "- `__undef__`\n",
    "- `_contrib_conv2d_NCHWc`\n",
    "- `_conv2d_grad`\n",
    "- `_max_pool2d_grad`\n",
    "- `avg_pool2d`\n",
    "- `batch_norm`\n",
    "- `broadcast_add`\n",
    "- `broadcast_div`\n",
    "- `broadcast_mul`\n",
    "- `broadcast_sub`\n",
    "- `broadcast_to`\n",
    "- `cast`\n",
    "- `concatenate`\n",
    "- `conv2d_transpose`\n",
    "- `dropout`\n",
    "- `flip`\n",
    "- `full`\n",
    "- `global_avg_pool2d`\n",
    "- `global_max_pool2d`\n",
    "- `multibox_transform_loc`\n",
    "- `nms`\n",
    "- `ones`\n",
    "- `pad`\n",
    "- `prelu`\n",
    "- `resize`\n",
    "- `split`\n",
    "- `tvm_op`\n",
    "- `upsampling`\n",
    "- `yolo2_reorg`\n",
    "- `zeros`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
