{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnvm.compiler\n",
    "import nnvm.symbol as sym\n",
    "import tvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm_lower_old = tvm.lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = {}\n",
    "\n",
    "def my_modified_lower(sch,\n",
    "                      args,\n",
    "                      name=\"default_function\",\n",
    "                      binds=None,\n",
    "                      simple_mode=False):\n",
    "    # simple mode is usually used for debugging, don't debug debugging\n",
    "    if not simple_mode:\n",
    "        calls[name] = {'sch': sch, 'args': args, 'binds': binds}\n",
    "        print(\"Lowering \" + name)\n",
    "        res = tvm_lower_old(sch, args, name, binds, simple_mode)\n",
    "        # Ok, we can't really print res (I haven't found how at least), so we do the same lowering twice\n",
    "        # simple_mode=True means that we don't wrap theresult in an api-ready function, and we don't apply loop partitioning\n",
    "        print(\"result:\\n\" + str(tvm_lower_old(sch, args, name, binds, simple_mode=True)))\n",
    "        print(\"\")\n",
    "        return res\n",
    "    else:\n",
    "        return tvm_lower_old(sch, args, name, binds, simple_mode)\n",
    "\n",
    "tvm.lower = my_modified_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(%x, %conv2d1_weight, %conv2d1_bias) {\n",
      "  %3 = conv2d(%x, %conv2d1_weight, %conv2d1_bias, channels='13', padding='[0, 0]', kernel_size='(1, 1)')\n",
      "  %4 = relu(%3)\n",
      "  %5 = broadcast_add(%x, %4)\n",
      "  %6 = sum(%5, keepdims='True')\n",
      "  ret %6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x = sym.Variable(\"x\", shape=(1,13,10000,10000), dtype=1)\n",
    "z = x\n",
    "z = sym.conv2d(data=z, channels=13, kernel_size=(1,1), padding=[0,0])\n",
    "z = sym.relu(data=z)\n",
    "z = x + z\n",
    "# TODO: Doesn't work without keepdims which might be a bug in nnvm, investigate\n",
    "z = sym.sum(z, keepdims=True)\n",
    "\n",
    "graph = nnvm.graph.create(z)\n",
    "print(graph.ir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowering fuse_conv2d_relu_broadcast_add_1\n",
      "result:\n",
      "// attr [pad_temp] storage_scope = \"global\"\n",
      "allocate pad_temp[float64 * 1 * 13 * 10000 * 10000]\n",
      "// attr [compute] storage_scope = \"global\"\n",
      "allocate compute[float64 * 1 * 13 * 10000 * 10000]\n",
      "produce pad_temp {\n",
      "  parallel (i0.i1.fused, 0, 13) {\n",
      "    for (i2, 0, 10000) {\n",
      "      for (i3, 0, 10000) {\n",
      "        pad_temp[((((i0.i1.fused*10000) + i2)*10000) + i3)] = input0[((((i0.i1.fused*10000) + i2)*10000) + i3)]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce compute {\n",
      "  parallel (nn.ff.fused, 0, 13) {\n",
      "    for (yy.init, 0, 10000) {\n",
      "      for (xx.outer.init, 0, 625) {\n",
      "        compute[ramp((((((nn.ff.fused*10000) + yy.init)*625) + xx.outer.init)*16), 1, 16)] = x16(0.000000)\n",
      "      }\n",
      "    }\n",
      "    for (rc, 0, 13) {\n",
      "      for (yy, 0, 10000) {\n",
      "        for (xx.outer, 0, 625) {\n",
      "          compute[ramp((((((nn.ff.fused*10000) + yy)*625) + xx.outer)*16), 1, 16)] = (compute[ramp((((((nn.ff.fused*10000) + yy)*625) + xx.outer)*16), 1, 16)] + (pad_temp[ramp((((((rc*10000) + yy)*625) + xx.outer)*16), 1, 16)]*x16(input1[((nn.ff.fused*13) + rc)])))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce tensor {\n",
      "  parallel (ax0.ax1.fused, 0, 13) {\n",
      "    for (ax2, 0, 10000) {\n",
      "      tensor[ramp((((ax0.ax1.fused*10000) + ax2)*10000), 1, 10000)] = (max((compute[ramp((((ax0.ax1.fused*10000) + ax2)*10000), 1, 10000)] + x10000(input2[ax0.ax1.fused])), x10000(0.000000)) + input0[ramp((((ax0.ax1.fused*10000) + ax2)*10000), 1, 10000)])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Lowering fuse_sum_1\n",
      "result:\n",
      "produce input0_red {\n",
      "  input0_red[0] = 0.000000\n",
      "  for (k1, 0, 13) {\n",
      "    for (k2, 0, 10000) {\n",
      "      for (k3, 0, 10000) {\n",
      "        input0_red[0] = (input0_red[0] + input0[((((k1*10000) + k2)*10000) + k3)])\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph._set_json_attr(\"target\", \"llvm\", \"str\")\n",
    "with tvm.target.create(\"llvm\"):\n",
    "    compiled = graph.apply('InferType').apply('InferShape').apply('GraphFusePartition').apply('GraphFuseCompile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = calls['fuse_sum']['sch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = sch.normalize()\n",
    "stmt = tvm.schedule.ScheduleOps(sch, tvm.schedule.InferBound(sch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "// attr [compute(input0_red, 0x217d190)] realize_scope = \"\"\n",
       "realize input0_red([0, 1], [0, 1], [0, 1], [0, 1]) {\n",
       "  produce input0_red {\n",
       "    input0_red(0, 0, 0, 0) =0.000000\n",
       "    for (k1, 0, 13) {\n",
       "      for (k2, 0, 100) {\n",
       "        for (k3, 0, 100) {\n",
       "          input0_red(0, 0, 0, 0) =(input0_red(0, 0, 0, 0) + input0(0, k1, k2, k3))\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm.ir_pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
