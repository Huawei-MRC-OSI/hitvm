{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnvm.compiler\n",
    "import nnvm.symbol as sym\n",
    "import tvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm_lower_old = tvm.lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = {}\n",
    "\n",
    "def my_modified_lower(sch,\n",
    "                      args,\n",
    "                      name=\"default_function\",\n",
    "                      binds=None,\n",
    "                      simple_mode=False):\n",
    "    # simple mode is usually used for debugging, don't debug debugging\n",
    "    if not simple_mode:\n",
    "        calls[name] = {'sch': sch, 'args': args, 'binds': binds}\n",
    "        print(\"Lowering \" + name)\n",
    "        res = tvm_lower_old(sch, args, name, binds, simple_mode)\n",
    "        # Ok, we can't really print res (I haven't found how at least), so we do the same lowering twice\n",
    "        # simple_mode=True means that we don't wrap theresult in an api-ready function, and we don't apply loop partitioning\n",
    "        print(\"result:\\n\" + str(tvm_lower_old(sch, args, name, binds, simple_mode=True)))\n",
    "        print(\"\")\n",
    "        return res\n",
    "    else:\n",
    "        return tvm_lower_old(sch, args, name, binds, simple_mode)\n",
    "\n",
    "tvm.lower = my_modified_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(%x, %conv2d0_weight, %conv2d0_bias) {\n",
      "  %3 = conv2d(%x, %conv2d0_weight, %conv2d0_bias, channels='13', padding='[0, 0]', kernel_size='(1, 1)')\n",
      "  %4 = relu(%3)\n",
      "  %5 = broadcast_add(%x, %4)\n",
      "  %6 = sum(%5, keepdims='True')\n",
      "  ret %6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x = sym.Variable(\"x\", shape=(1,13,100,100), dtype=1)\n",
    "z = x\n",
    "z = sym.conv2d(data=z, channels=13, kernel_size=(1,1), padding=[0,0])\n",
    "z = sym.relu(data=z)\n",
    "z = x + z\n",
    "# TODO: Doesn't work without keepdims which might be a bug in nnvm, investigate\n",
    "z = sym.sum(z, keepdims=True)\n",
    "\n",
    "graph = nnvm.graph.create(z)\n",
    "print(graph.ir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowering fuse_conv2d_relu_broadcast_add\n",
      "result:\n",
      "// attr [pad_temp] storage_scope = \"global\"\n",
      "allocate pad_temp[float64 * 1 * 13 * 100 * 100]\n",
      "// attr [compute] storage_scope = \"global\"\n",
      "allocate compute[float64 * 1 * 13 * 100 * 100]\n",
      "produce pad_temp {\n",
      "  parallel (i0.i1.fused, 0, 13) {\n",
      "    for (i2, 0, 100) {\n",
      "      for (i3, 0, 100) {\n",
      "        pad_temp[((((i0.i1.fused*100) + i2)*100) + i3)] = input0[((((i0.i1.fused*100) + i2)*100) + i3)]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce compute {\n",
      "  parallel (nn.ff.fused, 0, 13) {\n",
      "    for (yy.init, 0, 100) {\n",
      "      for (xx.outer.init, 0, 7) {\n",
      "        for (xx.inner.init.s, 0, 16) {\n",
      "          if (likely(((xx.outer.init*16) < (100 - xx.inner.init.s)))) {\n",
      "            compute[(((((nn.ff.fused*100) + yy.init)*100) + (xx.outer.init*16)) + xx.inner.init.s)] = 0.000000\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (rc, 0, 13) {\n",
      "      for (yy, 0, 100) {\n",
      "        for (xx.outer, 0, 7) {\n",
      "          for (xx.inner.s, 0, 16) {\n",
      "            if (likely(((xx.outer*16) < (100 - xx.inner.s)))) {\n",
      "              compute[(((((nn.ff.fused*100) + yy)*100) + (xx.outer*16)) + xx.inner.s)] = (compute[(((((nn.ff.fused*100) + yy)*100) + (xx.outer*16)) + xx.inner.s)] + (pad_temp[(((((rc*100) + yy)*100) + (xx.outer*16)) + xx.inner.s)]*input1[((nn.ff.fused*13) + rc)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce tensor {\n",
      "  parallel (ax0.ax1.fused, 0, 13) {\n",
      "    for (ax2, 0, 100) {\n",
      "      tensor[ramp((((ax0.ax1.fused*100) + ax2)*100), 1, 100)] = (max((compute[ramp((((ax0.ax1.fused*100) + ax2)*100), 1, 100)] + x100(input2[ax0.ax1.fused])), x100(0.000000)) + input0[ramp((((ax0.ax1.fused*100) + ax2)*100), 1, 100)])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Lowering fuse_sum\n",
      "result:\n",
      "produce input0_red {\n",
      "  input0_red[0] = 0.000000\n",
      "  for (k1, 0, 13) {\n",
      "    for (k2, 0, 100) {\n",
      "      for (k3, 0, 100) {\n",
      "        input0_red[0] = (input0_red[0] + input0[((((k1*100) + k2)*100) + k3)])\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph._set_json_attr(\"target\", \"llvm\", \"str\")\n",
    "with tvm.target.create(\"llvm\"):\n",
    "    compiled = graph.apply('InferType').apply('InferShape').apply('GraphFusePartition').apply('GraphFuseCompile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = calls['fuse_conv2d_relu_broadcast_add']['sch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
