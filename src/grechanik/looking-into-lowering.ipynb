{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnvm.compiler\n",
    "import nnvm.symbol as sym\n",
    "import tvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm_lower_old = tvm.lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = {}\n",
    "\n",
    "def my_modified_lower(sch,\n",
    "                      args,\n",
    "                      name=\"default_function\",\n",
    "                      binds=None,\n",
    "                      simple_mode=False):\n",
    "    # simple mode is usually used for debugging, don't debug debugging\n",
    "    if not simple_mode:\n",
    "        calls[name] = {'sch': sch, 'args': args, 'binds': binds}\n",
    "        print(\"Lowering \" + name)\n",
    "        res = tvm_lower_old(sch, args, name, binds, simple_mode)\n",
    "        # Ok, we can't really print res (I haven't found how at least), so we do the same lowering twice\n",
    "        # simple_mode=True means that we don't wrap theresult in an api-ready function, and we don't apply loop partitioning\n",
    "        print(\"result:\\n\" + str(tvm_lower_old(sch, args, name, binds, simple_mode=True)))\n",
    "        print(\"\")\n",
    "        return res\n",
    "    else:\n",
    "        return tvm_lower_old(sch, args, name, binds, simple_mode)\n",
    "\n",
    "tvm.lower = my_modified_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(%x, %conv2d0_weight, %conv2d0_bias) {\n",
      "  %3 = conv2d(%x, %conv2d0_weight, %conv2d0_bias, channels='13', padding='[0, 0]', kernel_size='(1, 1)')\n",
      "  %4 = relu(%3)\n",
      "  %5 = broadcast_add(%x, %4)\n",
      "  %6 = sum(%5, keepdims='True')\n",
      "  ret %6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "x = sym.Variable(\"x\", shape=(1,13,10000,10000), dtype=1)\n",
    "z = x\n",
    "z = sym.conv2d(data=z, channels=13, kernel_size=(1,1), padding=[0,0])\n",
    "z = sym.relu(data=z)\n",
    "z = x + z\n",
    "# TODO: Doesn't work without keepdims which might be a bug in nnvm, investigate\n",
    "z = sym.sum(z, keepdims=True)\n",
    "\n",
    "graph = nnvm.graph.create(z)\n",
    "print(graph.ir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To dump results of tvm ir passes (it will create a lot of files), use \n",
    "\n",
    "    with tvm.build_config(dump_pass_ir=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowering fuse_conv2d_relu_broadcast_add\n",
      "result:\n",
      "// attr [pad_temp] storage_scope = \"global\"\n",
      "allocate pad_temp[float64 * 1 * 13 * 10000 * 10000]\n",
      "// attr [compute] storage_scope = \"global\"\n",
      "allocate compute[float64 * 1 * 13 * 10000 * 10000]\n",
      "produce pad_temp {\n",
      "  parallel (i0.i1.fused, 0, 13) {\n",
      "    for (i2, 0, 10000) {\n",
      "      for (i3, 0, 10000) {\n",
      "        pad_temp[((((i0.i1.fused*10000) + i2)*10000) + i3)] = input0[((((i0.i1.fused*10000) + i2)*10000) + i3)]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce compute {\n",
      "  parallel (nn.ff.fused, 0, 13) {\n",
      "    for (yy.init, 0, 10000) {\n",
      "      for (xx.outer.init, 0, 625) {\n",
      "        compute[ramp((((((nn.ff.fused*10000) + yy.init)*625) + xx.outer.init)*16), 1, 16)] = x16(0.000000)\n",
      "      }\n",
      "    }\n",
      "    for (rc, 0, 13) {\n",
      "      for (yy, 0, 10000) {\n",
      "        for (xx.outer, 0, 625) {\n",
      "          compute[ramp((((((nn.ff.fused*10000) + yy)*625) + xx.outer)*16), 1, 16)] = (compute[ramp((((((nn.ff.fused*10000) + yy)*625) + xx.outer)*16), 1, 16)] + (pad_temp[ramp((((((rc*10000) + yy)*625) + xx.outer)*16), 1, 16)]*x16(input1[((nn.ff.fused*13) + rc)])))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "produce tensor {\n",
      "  parallel (ax0.ax1.fused, 0, 13) {\n",
      "    for (ax2, 0, 10000) {\n",
      "      tensor[ramp((((ax0.ax1.fused*10000) + ax2)*10000), 1, 10000)] = (max((compute[ramp((((ax0.ax1.fused*10000) + ax2)*10000), 1, 10000)] + x10000(input2[ax0.ax1.fused])), x10000(0.000000)) + input0[ramp((((ax0.ax1.fused*10000) + ax2)*10000), 1, 10000)])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Lowering fuse_sum\n",
      "result:\n",
      "produce input0_red {\n",
      "  input0_red[0] = 0.000000\n",
      "  for (k1, 0, 13) {\n",
      "    for (k2, 0, 10000) {\n",
      "      for (k3, 0, 10000) {\n",
      "        input0_red[0] = (input0_red[0] + input0[((((k1*10000) + k2)*10000) + k3)])\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph._set_json_attr(\"target\", \"llvm\", \"str\")\n",
    "with tvm.target.create(\"llvm\"):\n",
    "    compiled = graph.apply('InferType').apply('InferShape').apply('GraphFusePartition').apply('GraphFuseCompile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_ScheduleOps_ir.cc              51_UnrollLoop_ir.cc\r\n",
      "10_Simplify_ir.cc                52_Simplify_ir.cc\r\n",
      "11_LowerStorageAccessInfo_ir.cc  53_LowerStorageAccessInfo_ir.cc\r\n",
      "12_RemoveNoOp_ir.cc              54_RemoveNoOp_ir.cc\r\n",
      "13_RewriteUnsafeSelect_ir.cc     55_RewriteUnsafeSelect_ir.cc\r\n",
      "14_MakeAPI_ir.cc                 56_ThreadSync_ir.cc\r\n",
      "15_ScheduleOps_ir.cc             57_ThreadSync_ir.cc\r\n",
      "16_InjectPrefetch_ir.cc          58_LowerThreadAllreduce_ir.cc\r\n",
      "17_StorageFlatten_ir.cc          59_SplitHostDevice_ir.cc\r\n",
      "18_CanonicalSimplify_ir.cc       5_VectorizeLoop_ir.cc\r\n",
      "19_VectorizeLoop_ir.cc           60_ThreadSync_ir.cc\r\n",
      "1_InjectPrefetch_ir.cc           61_ThreadSync_ir.cc\r\n",
      "20_InjectVirtualThread_ir.cc     62_LowerThreadAllreduce_ir.cc\r\n",
      "21_InjectDoubleBuffer_ir.cc      63_SplitHostDevice_ir.cc\r\n",
      "22_StorageRewrite_ir.cc          64_BindDeviceType_ir.cc\r\n",
      "23_UnrollLoop_ir.cc              65_BindDeviceType_ir.cc\r\n",
      "24_Simplify_ir.cc                66_LowerTVMBuiltin_ir.cc\r\n",
      "25_LowerStorageAccessInfo_ir.cc  67_LowerTVMBuiltin_ir.cc\r\n",
      "26_RemoveNoOp_ir.cc              68_LowerIntrin_ir.cc\r\n",
      "27_RewriteUnsafeSelect_ir.cc     69_LowerIntrin_ir.cc\r\n",
      "28_ScheduleOps_ir.cc             6_InjectVirtualThread_ir.cc\r\n",
      "29_InjectPrefetch_ir.cc          70_CombineContextCall_ir.cc\r\n",
      "2_StorageFlatten_ir.cc           71_CombineContextCall_ir.cc\r\n",
      "30_StorageFlatten_ir.cc          72_ThreadSync_ir.cc\r\n",
      "31_CanonicalSimplify_ir.cc       73_ThreadSync_ir.cc\r\n",
      "32_LoopPartition_ir.cc           74_LowerThreadAllreduce_ir.cc\r\n",
      "33_VectorizeLoop_ir.cc           75_SplitHostDevice_ir.cc\r\n",
      "34_InjectVirtualThread_ir.cc     76_ThreadSync_ir.cc\r\n",
      "35_InjectDoubleBuffer_ir.cc      77_ThreadSync_ir.cc\r\n",
      "36_StorageRewrite_ir.cc          78_LowerThreadAllreduce_ir.cc\r\n",
      "37_UnrollLoop_ir.cc              79_SplitHostDevice_ir.cc\r\n",
      "38_Simplify_ir.cc                7_InjectDoubleBuffer_ir.cc\r\n",
      "39_LowerStorageAccessInfo_ir.cc  80_BindDeviceType_ir.cc\r\n",
      "3_CanonicalSimplify_ir.cc        81_BindDeviceType_ir.cc\r\n",
      "40_RemoveNoOp_ir.cc              82_LowerTVMBuiltin_ir.cc\r\n",
      "41_RewriteUnsafeSelect_ir.cc     83_LowerTVMBuiltin_ir.cc\r\n",
      "42_MakeAPI_ir.cc                 84_LowerIntrin_ir.cc\r\n",
      "43_ScheduleOps_ir.cc             85_LowerIntrin_ir.cc\r\n",
      "44_InjectPrefetch_ir.cc          86_CombineContextCall_ir.cc\r\n",
      "45_StorageFlatten_ir.cc          87_CombineContextCall_ir.cc\r\n",
      "46_CanonicalSimplify_ir.cc       8_StorageRewrite_ir.cc\r\n",
      "47_VectorizeLoop_ir.cc           9_UnrollLoop_ir.cc\r\n",
      "48_InjectVirtualThread_ir.cc     compiling-nnvm-graph.ipynb\r\n",
      "49_InjectDoubleBuffer_ir.cc      looking-into-lowering.ipynb\r\n",
      "4_LoopPartition_ir.cc            nnvm-overview.pptx\r\n",
      "50_StorageRewrite_ir.cc          schedule_primitives.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fuse_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3ddb93309f94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fuse_sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'fuse_sum'"
     ]
    }
   ],
   "source": [
    "sch = calls['fuse_sum']['sch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = sch.normalize()\n",
    "stmt = tvm.schedule.ScheduleOps(sch, tvm.schedule.InferBound(sch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "// attr [compute(input0_red, 0x217d190)] realize_scope = \"\"\n",
       "realize input0_red([0, 1], [0, 1], [0, 1], [0, 1]) {\n",
       "  produce input0_red {\n",
       "    input0_red(0, 0, 0, 0) =0.000000\n",
       "    for (k1, 0, 13) {\n",
       "      for (k2, 0, 100) {\n",
       "        for (k3, 0, 100) {\n",
       "          input0_red(0, 0, 0, 0) =(input0_red(0, 0, 0, 0) + input0(0, k1, k2, k3))\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvm.ir_pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
