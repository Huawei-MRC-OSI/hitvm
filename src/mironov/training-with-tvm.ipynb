{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training some neural net on mnist with pure tvm\n",
    "\n",
    "Here we are going to train a neural net in pure tvm using tvm-level automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topi\n",
    "import tvm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topi.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/p3i9s3vhjskbrnfl97fd7b0vmn7bqddh-python3.6-h5py-2.7.1/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch generator. The last incomplete batch is thrown out because nnvm uses fixed batch size. We use the same function for keras so that the training results are closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(x, y):\n",
    "    for i in range(int(x.shape[0] / batch_size)):\n",
    "        yield (x[i:i+batch_size, ...].astype('float32'),\n",
    "               y[i:i+batch_size, ...].astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "This is the keras definition of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = keras.models.Sequential()\n",
    "keras_model.add(keras.layers.Reshape((28, 28, 1), input_shape=(28, 28)))\n",
    "keras_model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu'))\n",
    "keras_model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "keras_model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "keras_model.add(keras.layers.Flatten())\n",
    "keras_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "keras_model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "keras_model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=keras.optimizers.SGD(lr=1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same thing written in tvm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tvm.placeholder((batch_size, 28, 28))\n",
    "y = tvm.placeholder((batch_size, num_classes))\n",
    "\n",
    "w1 = tvm.placeholder((32, 1, 3, 3))\n",
    "b1 = tvm.placeholder((32,))\n",
    "w2 = tvm.placeholder((64, 32, 3, 3))\n",
    "b2 = tvm.placeholder((64,))\n",
    "w3 = tvm.placeholder((128, 9216))\n",
    "b3 = tvm.placeholder((128,))\n",
    "w4 = tvm.placeholder((num_classes, 128))\n",
    "b4 = tvm.placeholder((num_classes,))\n",
    "\n",
    "t = topi.reshape(x, (batch_size, 1, 28, 28))\n",
    "t = topi.nn.relu(topi.nn.conv2d(t, w1, 1, 0) + topi.reshape(b1, (1, 32, 1, 1)))\n",
    "t = topi.nn.relu(topi.nn.conv2d(t, w2, 1, 0) + topi.reshape(b2, (1, 64, 1, 1)))\n",
    "t = topi.nn.pool(t, [2, 2], [2, 2], [0, 0, 0, 0], 'max')\n",
    "t = topi.nn.flatten(t)\n",
    "t = topi.nn.relu(topi.nn.dense(t, w3, b3))\n",
    "t = topi.nn.dense(t, w4, b4)\n",
    "\n",
    "predictions = topi.nn.softmax(t)\n",
    "loss = - topi.sum(y * topi.nn.log_softmax(t)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [w1, b1, w2, b2, w3, b3, w4, b4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = topi.full((1,), 'float32', 1.0)\n",
    "gradients = [tvm.ir_pass.JacobianRecursive(loss, w, head) for w in weights]\n",
    "learning_rate = tvm.placeholder(())\n",
    "new_weights = [w - learning_rate*g for w, g in zip(weights, gradients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and initializing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(tensor):\n",
    "    return [s.value for s in tensor.shape]\n",
    "\n",
    "def empty_val(tensor):\n",
    "    if isinstance(tensor, list):\n",
    "        return [empty_val(t) for t in tensor]\n",
    "    else:\n",
    "        return tvm.nd.empty(get_shape(tensor), tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_values = empty_val(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_module = tvm.build(tvm.create_schedule(loss.op), [loss, x, y] + weights)\n",
    "\n",
    "def tvm_test(xval, yval):\n",
    "    args = [empty_val(loss)] + [tvm.ndarray.array(xval), tvm.ndarray.array(yval)] + weights_values\n",
    "    testing_module(*args)\n",
    "    return args[0].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(nan, dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx, yy = next(batches(x_train, y_train))\n",
    "tvm_test(xx, yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_cgraph, training_libmod, training_params = nnvm.compiler.build(training_graph, 'llvm')\n",
    "training_module = tvm.contrib.graph_runtime.create(training_cgraph, training_libmod, tvm.cpu(0))\n",
    "\n",
    "if training_params:\n",
    "    training_module.set_input(**training_params)\n",
    "\n",
    "testing_cgraph, testing_libmod, testing_params = nnvm.compiler.build(testing_graph, 'llvm')\n",
    "testing_module = tvm.contrib.graph_runtime.create(testing_cgraph, testing_libmod, tvm.cpu(0))\n",
    "\n",
    "if testing_params:\n",
    "    testing_module.set_input(**testing_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly initialize weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing y [128, 10]\n",
      "Initializing x [128, 28, 28]\n",
      "Initializing dense0_weight [544, 784]\n",
      "Initializing dense0_bias [544]\n",
      "Initializing dense1_weight [512, 544]\n",
      "Initializing dense1_bias [512]\n",
      "Initializing dense2_weight [10, 512]\n",
      "Initializing dense2_bias [10]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "shapes = training_graph.apply('InferShape').json_attr('shape')\n",
    "    \n",
    "for v in loss.list_input_variables():\n",
    "    shape = shapes[training_graph.index.node_id(v.attr('name'))]\n",
    "    print(\"Initializing \" + str(v.attr('name')) + \" \" + str(shape))\n",
    "    if 'bias' in str(v.attr('name')):\n",
    "        training_module.set_input(v.attr('name'), np.zeros(shape).astype('float32'))\n",
    "    else:\n",
    "        training_module.set_input(v.attr('name'), np.random.normal(scale=0.05, size=shape).astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary functions, one returns the weights from the training graph, the other assigns these weights to the keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights():\n",
    "    for v in weight_vars:\n",
    "        shape = shapes[training_graph.index.node_id(v.attr('name'))]\n",
    "        yield v.attr('name'), training_module.get_input(v.attr('name'), tvm.nd.empty(shape))\n",
    "\n",
    "def assign_nnvm_weights_to_keras():\n",
    "    keras_model.set_weights([np.transpose(v.asnumpy()) for _, v in get_weights()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the reference keras model\n",
    "\n",
    "Let's first train the reference keras model. We will use the initial weights from the nnvm graph to make comparison fairer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "468/468 [==============================] - 47s 100ms/step - loss: 0.4754\n",
      "keras test loss: 0.40138487846820387\n"
     ]
    }
   ],
   "source": [
    "keras_model.fit_generator(batches(x_train, y_train), steps_per_epoch=int(len(x_train) / batch_size))\n",
    "\n",
    "test_loss = 0\n",
    "for step, (xs, ys) in enumerate(batches(x_test, y_test)):\n",
    "    test_loss += keras_model.test_on_batch(xs, ys)\n",
    "        \n",
    "test_loss /= step\n",
    "\n",
    "print(\"keras test loss: {}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the nnvm model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the nnvm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen: 59008  train loss: 0.46966883540153503\n",
      "CPU times: user 2min 8s, sys: 5min 57s, total: 8min 6s\n",
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seen = 0\n",
    "for step, (xs, ys) in enumerate(batches(x_train, y_train)):\n",
    "    # load data\n",
    "    training_module.set_input('x', xs)\n",
    "    training_module.set_input('y', ys)\n",
    "    # run a training step\n",
    "    training_module.run()\n",
    "    \n",
    "    seen += xs.shape[0]\n",
    "    train_loss = training_module.get_output(0, tvm.nd.empty((1,))).asnumpy()[0]\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(\"seen: {}  train loss: {}\".format(seen, train_loss), end='\\r')\n",
    "        \n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing  the nnvm model\n",
    "\n",
    "Move weights to the testing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, val in get_weights():\n",
    "    testing_module.set_input(name, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute loss on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.6421400455685405\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "for step, (xs, ys) in enumerate(batches(x_test, y_test)):\n",
    "    testing_module.set_input('x', xs)\n",
    "    testing_module.set_input('y', ys)\n",
    "    testing_module.run()\n",
    "    \n",
    "    test_loss += testing_module.get_output(0, tvm.nd.empty((1,))).asnumpy()[0]\n",
    "        \n",
    "test_loss /= step\n",
    "print(\"test loss: {}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that we compute everything correctly, move nnvm weights to the keras model and compute the test loss using keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_nnvm_weights_to_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.6421400262163831\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "for step, (xs, ys) in enumerate(batches(x_test, y_test)):\n",
    "    test_loss += keras_model.test_on_batch(xs, ys)\n",
    "        \n",
    "test_loss /= step\n",
    "print(\"test loss: {}\".format(test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
